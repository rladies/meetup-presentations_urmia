---
title: "Cluster analysis"
author: "Naghmeh Pakgohar"
date: "2023-11-07"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

#install.packages(c("cluster", "factoextra"))

```{r }
library(cluster)
library(factoextra)

```

## Importing your data into R 
```{r pressure, echo=FALSE}
data("USArrests") # Loading
head(USArrests, 3) # Print the first 3 rows

```
## Prepration your data
```{r}
df <- USArrests # Use df as shorter name
df <- na.omit(df)#remove any missing value
df <- scale(df)#standardizing the data (scaled = (x – x̄) / s)
head(df, n = 3)
```

#Euclidean Distance matrix computation
```{r}
set.seed(123)
ss <- sample(1:50, 15) # Take 15 random rows
df <- USArrests[ss, ] # Subset the 15 rows
df.scaled <- scale(df) # Standardize the variables
# Eudliean distance 
dist.eucl <- dist(df.scaled, method = "euclidean") 
# Reformat as a matrix
# Subset the first 3 columns and rows and Round the values
round(as.matrix(dist.eucl)[1:3, 1:3], 1)
fviz_dist(dist.eucl) #Visualizing distance matrices
##Red: high similarity (ie: low dissimilarity) | Blue: low similarity

```


#Pearson correlation distance matrix computation
```{r}
dist.cor <- get_dist(df.scaled, method = "pearson")
# Display a subset
round(as.matrix(dist.cor)[1:3, 1:3], 1)
```

#Computing distances for mixed data
```{r}
data(flower)
head(flower, 3)
str(flower)
dd <- daisy(flower) #Gower’s metric
round(as.matrix(dd)[1:3, 1:3], 2)
```

##Estimating the optimal number of clusters
```{r}
#Estimating the optimal number of clusters
fviz_nbclust(df, kmeans, method = "wss") +
geom_vline(xintercept = 4, linetype = 2)

fviz_nbclust(df, kmeans, method = "silhouette")+
theme_classic()
```


##Compute k-means with k = 4
```{r}
set.seed(123)
km.res <- kmeans(df, 4, nstart = 25)
# Print the results
print(km.res)
dd <- cbind(df, cluster = km.res$cluster)
head(dd)

# Cluster number for each of the observations
km.res$cluster
# Cluster size
km.res$size
# Cluster means
km.res$centers
#Visualizing k-means clusters
fviz_cluster(km.res, data = df,
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
ellipse.type = "euclid", # Concentration ellipse
star.plot = TRUE, # Add segments from centroids to items
repel = TRUE, # Avoid label overplotting (slow)
ggtheme = theme_minimal())

```

#Hierarchical Clustering
```{r}
#Agglomerative Clustering
res.dist <- dist(df, method = "euclidean")

#Linkage methids-“ward.D2”, “single”,,“complete”, “average”, “mcquitty”, #“median” or “centroid”.
res.hc <- hclust(d = res.dist, method = "ward.D2")
fviz_dend(res.hc, cex = 0.5)
# Cut tree into 4 groups
grp <- cutree(res.hc, k = 4)
head(grp, n = 4)
# Number of members in each cluster
table(grp)

fviz_dend(res.hc, k = 4, # Cut in four groups
cex = 0.5, # label size
k_colors = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
color_labels_by_k = TRUE, # color labels by groups
rect = TRUE # Add rectangle around groups
)

fviz_cluster(list(data = df, cluster = grp),
palette = c("#2E9FDF", "#00AFBB", "#E7B800", "#FC4E07"),
ellipse.type = "convex", # Concentration ellipse
repel = TRUE, # Avoid label overplotting (slow)
show.clust.cent = FALSE, ggtheme = theme_minimal())
```

##Reference

```{r}
#Pakgohar, N., Eshaghi Rad, J., Gholami, G., Alijanpour, A., & Roberts, D. W. (2021). A comparative study of hard clustering algorithms for vegetation data. Journal of Vegetation Science, 32(3), e13042.
#Pakgohar, N., Lengyel, A., & Botta-Dukát, Z. (2023). Quantitative evaluation of internal clustering validation indices using binary datesets. bioRxiv, 2023-08.
#Kassambara, A. (2017). Practical guide to cluster analysis in R: Unsupervised machine learning (Vol. 1). Sthda.
```

---
 dev. off()


